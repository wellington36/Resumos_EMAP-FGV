\documentclass{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{commath}
\usepackage[shortlabels]{enumitem}
\usepackage{hyperref}
\usepackage{mathtools}
\title{Álgebra Linear}
\author{André Ribeiro}
\date{June 2022}

\newcommand{\K}{\mathbb{K}}
\newcommand{\inprod}[2]{\langle {#1}, {#2} \rangle}
\renewcommand{\L}{\mathcal{L}}
\DeclareMathOperator{\Span}{span}

\hypersetup{
    colorlinks=true,
    linkcolor=magenta,
    urlcolor=cyan}

\begin{document}

\maketitle

\tableofcontents

\newpage

\section{Espaços Vetoriais}

Dado um conjunto $V$ e um corpo $(\K, +, \cdot)$, dizemos que V é um $\K$-espaço vetorial se os seguintes axiomas são satisfeitos:

\begin{itemize}
    \item O operador
    \begin{align*}
        +_{_V} : V \times V &\to V \\
              (u, v) &\mapsto u +_{_V} v
    \end{align*} 
    é associativo, comutativo, possui elemento neutro e inverso.
    \item O operador 
    \begin{align*}
        \cdot_{_V} : \K \times V &\to V \\
                   (a, u) &\mapsto a \cdot_{_V} u
    \end{align*} 
    é compatível com o produto em $\K$ e possui elemento neutro. Este operador é também distributivo com respeito a $+_{_V}$ e $+$, i.e., dados $a, b \in \K$ e $u, v \in V$, temos que \[ (a + b, v) \to (a, v) +_{_V} (b, v) \quad \land \quad (a, u +_{_V} v) \to (a, u) +_{_V} (a, v)  \]
\end{itemize}

A partir de agora usaremos as notações usuais de soma e produto, e o contexto deixa claro quando estamos tratando de somas e produtos no corpo ou nos espaços vetoriais.

\subsection{Subespaços Vetoriais}

Dado $V$ um $\K$-espaço vetorial, dizemos que $F \subset V$ é um $\K$-subespaço vetorial se valem as propriedades
\begin{itemize}
    \item $x, y \in F \implies x + y \in F$
    \item $\alpha \in \K,\: x \in F \implies \alpha x \in F$, 
\end{itemize} \vspace{3mm}
\textbf{Proposição}: Se $V$ é $\K$-espaço vetorial e $F \subset V$ é subespaço então $F$ é $\K$-espaço vetorial

\newpage

\subsection{Somas e Somas Diretas}

Sejam $U$ $\K$-espaço vetorial e $V_1, \cdots, V_n \subset U$ subespaços. Definimos o conjunto $V = \sum V_i$ como
\begin{align*}
    V   &= V_1 + V_2 + \cdots + V_n \\
        &= \{ v_1 + v_2 + \cdots + v_n \: | \: v_i \in V_i, \forall i = 1,2,\cdots,n \}
\end{align*}
O conjunto $V \subset U$ é também um subespaço vetorial. Se cada vetor de $V$ pode ser escrito unicamente como uma soma de vetores acima, dizemos que $V$ é soma direta de $V_1, \cdots V_n$, e denotamos por
\begin{align*}
    V = V_1 \oplus V_2 \oplus \cdots \oplus V_n
\end{align*} \vspace{5mm}
\textbf{Proposição}: Seja $V$ um $\K$-espaço vetorial e $U_1, \cdots U_n \subset V$ subespaços. Então $V = U_1 \oplus \cdots \oplus U_n$ se, e somente se
\begin{itemize}
    \item $V = U_1 + \cdots U_n$
    \item $0 = u_1 + \cdots u_n \iff u_1 = \cdots = u_n = 0$
\end{itemize} \vspace{5mm}
\textbf{Proposição}:  Seja $V$ um $\K$-espaço vetorial e $U, W \subset V$ subespaços. Então $V = U \oplus W$
\begin{itemize}
    \item $V = U + W$
    \item $U \cap W = \{0\}$
\end{itemize}

\newpage

\section{Espaços Vetoriais Finitos}

\subsection{Gerador (span)}

Seja $V$ um $\K$-espaço vetorial e $S = \{ x_1, \cdots, x_k \} \subset V$ subespaço, definimos o espaço gerado por $S$ como o menor espaço vetorial contendo $S$,  (i.e. o conjunto de todas as combinações lineares de seus vetores) denotado por
\begin{align*}
    \operatorname{Ger}(S) = \Span(S) = \left\{ \sum_{j=1}^k \alpha_jx_j,\: \alpha_j \in \K \right\}
\end{align*}
Dizemos que $V$ tem dimensão finita se $\exists E \in V$ t.q. $V = \Span(E)$, caso contrário $V$ tem dimensão infinita.

\subsection{Dependência e Independência Linear}

Seja $V$ um $\K$-espaço vetorial e $S = \{s_{i}\}_{1 \leq i \leq n}  \subset  V$ subespaço. Dizemos que o conjunto de vetores em $S$ é linearmente independente (LI) se
\begin{align*}
    a_1s_1 + a_2s_2 + \cdots + a_ns_n = 0 \iff a_1 = a_2 = \cdots = a_n = 0
\end{align*}
O conjunto de vetores é linearmente dependente (LD) se não é LI.

\subsection{Base}

Seja $V$ um $\K$-espaço vetorial. Um conjunto $B \subset V$ é uma base de $V$ se $V = \Span(B)$ e os vetores de $B$ são LI. \vspace{3mm} \\
\textbf{Corolário}: Todo espaço vetorial finito tem uma base. \vspace{3mm} \\
\textbf{Teorema}: Seja $V$ um $\K$-espaço vetorial. Se $B_1$ e $B_2$ são bases para $V$, então $\abs{B_1} = \abs{B_2}$

\subsection{Dimensão}

Seja $V$ um $\K$-espaço vetorial. Definimos a dimensão de $V$ sobre $\K$, $\dim_{\K} V$, como a cardinalidade de qualquer base de $V$.  \vspace{3mm} \\
\textbf{Teorema}: Sejam $U_1, U_2$ subespaços de um $\K$-espaço vetorial finito. Então
\begin{align*}
    \dim_{\K}(U_1 + U_2) = \dim(U_1) + \dim(U_2) - \dim(U_1 \cap U_2)
\end{align*}

\newpage

\section{Transformações Lineares}

Sejam $U$ e $W$ $\K$-espaços vetoriais. Uma transformação linear $T: U \to W$ é uma função que satisfaz, $\forall u,v \in U$ e $\forall \alpha \in \K$
\begin{itemize}
    \item $T(u + v) = T(u) + T(v)$
    \item $T(\alpha u) = \alpha(Tu)$
\end{itemize}
O conjunto das funções lineares de $U$ em $W$ é denotado por $\L(U,W)$

\subsection{Núcleo e Imagem}

Definimos o núcleo e a imagem (ou espaço nulo e espaço coluna) de uma transformação $T \in \L(U,W)$, respectivamente, como os conjuntos
\begin{align*}
    N(T) = \{ v \in U \: | \: Tv = 0 \} \quad \land \quad C(T) = \{ Tv \: | \: v \in U \}
\end{align*} \\
\textbf{Teorema}: $N(T) \subset U$ e $C(T) \subset W$ são subespaços. \vspace{3mm} \\
Um mapa linear $T: U \to W$ é sobrejetivo se $C(T) = W$ e é injetivo se, $\forall u, v \in U$, temos
\begin{align*}
    Tu = Tv \iff u = v
\end{align*} \\
\textbf{Proposição}: Seja $T \in \L(U,W)$. Então $T$ é injetiva se, e somente se, $N(T) = \nolinebreak\{0\}$ \vspace{3mm} \\
\textbf{Teorema}: Se $\dim_{\K} U < \infty$ e $T \in \L(U,W)$ então $\dim_{\K} C(T) < \infty$ e
\begin{align*}
    \dim_{\K} U = \dim_{\K} C(T) + \dim_{\K} N(T) 
\end{align*}
\textbf{Corolário}: Se $U$ e $W$ são $\K$-espaços com dimensão finita com $\dim U > \dim W$, então nenhum mapa linear de $U$ para $W$ é injetivo. Analogamente, se \linebreak $\dim U < \dim W$, então nenhum mapa linear de $U$ para $W$ é sobrejetivo.

\subsection{Matriz de uma transformação linear}

Seja $T \in \mathcal{L}(U,W)$, e suponha que $(u_1,\cdots,u_n)$ e $(w_1,\cdots,w_m)$ são bases de $U$ e $W$, respectivamente. Podemos então escrever cada $Tu_k$ unicamente como combinação linear dos $w_j$
\begin{align*}
    Tu_k = \sum_{j=1}^m a_{j,k}w_j
\end{align*}
com $k = 1,\cdots,n$ e $a_{j,k} \in \K,\:\forall j,k$. A matriz $m \times n$ cujas entradas são compostas pelos $a$'s é chamada matriz da transformação linear $T$, denotada por
\begin{align*}
    M(T,(u_1,\cdots,u_n),(w_1,\cdots,w_m)) = M(T) =
    \begin{bmatrix}
        a_{1,1} & \cdots & a_{1,n} \\
        \vdots & & \vdots \\
        a_{m,1} & \cdots & a_{m,n}
    \end{bmatrix}
\end{align*}

É fácil mostrar que, dados $T, S \in \mathcal{L}(U,W)$ e $c \in \K$, temos que
\begin{itemize}
    \item $M(T + S) = M(T) + M(S)$
    \item $M(cT) = cM(T)$
\end{itemize}
Seja $\mathbb{M}(m,n,\K) = \mathbb{M}_{m \times n}(\K) = \K^{m \times n}$ o conjunto das matrizes $m \times n$ com entradas em $\K$. Com as operações acima, o conjunto $\K^{m \times n}$ é um $\K$-espaço vetorial. Note que a matriz cujas entradas são todas $0$ representa a identidade aditiva desse espaço. Consideremos agora, um $\K$-espaço $V$, $(v_1,\cdots,v_p)$ uma base e as aplicações lineares $S: U \to V$, $T: V \to W$. Então $TS$ é uma aplicação de $U$ para $W$. Com a multiplicação usual de matrizes, temos também que
\begin{align*}
    M(TS) = M(T)M(S)
\end{align*}
Seja $(u_1,\cdots,u_n)$ uma base de $U$. Se $u \in U$, então
\begin{align*}
    u = \sum_{i=1}^n a_iu_i
\end{align*}
A matriz de $u$ é definida por
\begin{align*}
    M(u) = M(u,(u_1,\cdots,u_n)) = 
    \begin{bmatrix}
        a_1 \\
        \vdots \\
        a_n
    \end{bmatrix}
\end{align*}

\subsection{Aplicações inversíveis}

\textbf{Proposição}: Uma aplicação linear é invertível se, e somente se, é injetiva e sobrejetiva \vspace{3mm} \\
Dizemos que dois espaços vetoriais são isomorfos se existe uma bijeção entre eles, i.e. uma aplicação linear invertível de um espaço para o outro. \vspace{3mm} \\
\textbf{Teorema}: Dois espaços vetoriais finitos são isomorfos se, e se somente se, tem mesma dimensão. \vspace{3mm} \\
\textbf{Proposição}: A aplicação $M: \mathcal{L}(U,W) \to \K^{m \times n}$ é invertível, i.e. \linebreak $\mathcal{L}(U,W) \cong \K^{m \times n}$ \vspace{3mm} \\
Note que uma base para o espaço das matrizes $m \times n$ são as matrizes com 1 em uma entrada e 0 nas outras. Com isso, temos que $\dim \K^{m \times n} = mn$. Como esse espaço é isomorfo ao dos funcionais lineares, $\dim \mathcal{L}(U,W) = mn$. \vspace{3mm} \\
\textbf{Teorema}: Seja $U$ um $\K$-espaço de dimensão finita. Se $T \in \mathcal{L}(U)$, então as seguintes definições são equivalentes
\begin{enumerate}[(a)]
    \item T é invertível
    \item T é injetiva
    \item T é sobrejetiva
\end{enumerate}

\newpage

\section{Autovalores e Autovetores}

\subsection{Invariantes}

Sejam $V$ um $\K$-espaço, $T \in \mathcal{L}(V)$ e $U \subset V$ subespaço. Dizemos que $U$ é invariante sob $T$ se
\begin{align*}
    u \in U \implies Tu \in U,\:\forall u \in U
\end{align*}
Em outras palavras, $U$ é invariante sob $T$ se $T_{\mid U}$ é um operador em $U$. Note que $\{0\}$ e $V$ são invariantes triviais, e é fácil verificar que os espaços $N(T)$ e $C(T)$ são invariantes sob $T$. O caso mais simples de subespaço invariante é quando $\dim U_1 = 1$. Nesse caso, dado $u \in V$ temos
\begin{align*}
    U_1 = \{\alpha u \: | \: \alpha \in \K \}
\end{align*}
Temos que todos subespaços de dimensão 1 de $V$ são dessa forma. Se $U_1$ é invariante sob $T$, então
\begin{align*}
    Tu = \lambda u
\end{align*}
Esse caso particular tem enorme importância. Dizemos que um escalar $\lambda \in \K$ é um autovalor de $T \in \mathcal{L}(V)$ se $\exists u \in V \setminus \{0\}$ tal que $Tu = \lambda u$. Portanto $T$ tem subespaço invariante de dimensão 1 se, e somente se, possui um autovalor. Perceba que
\begin{align*}
    Tu = \lambda u \iff (T - I\lambda)u = 0
\end{align*}
portanto $\lambda$ é um autovalor se, e somente se $T - I\lambda$ não é invertível (i.e. não é bijetiva). O vetor $u$ que satisfaz essa equação é chamado autovetor de $T$ correspondente a $\lambda$. O conjunto de autovetores associados a $\lambda$ é dado por $N(T - I\lambda) \subset V$ \vspace{3mm} \\
\textbf{Teorema}: Seja $T \in \mathcal{L}(V)$. Suponha $\lambda_1, \cdots, \lambda_m$ são autovalores distintos de $T$ e $v_1, \cdots, v_m$ são os autovetores correspondentes (diferentes de 0). Então $(v_1, \cdots, v_m)$ é L.I. \vspace{3mm} \\
\textbf{Corolário}: Cada operador em $V$ tem no máximo $\dim V$ autovalores. \vspace{3mm} \\

\subsection{Polinômios aplicados a operadores}

Se $T \in \mathcal{L}(V)$, definimos
\begin{align*}
    T^m = \underbrace{T \cdots T}_{m \text{ vezes}}
\end{align*}
Por conveniência $T^0 = I$. Definimos também
\begin{align*}
    T^{-m} = (T^{-1})^m
\end{align*}
É fácil verificar que valem as propriedades
\begin{align*}
    T^nT^m = T^{n+m} \quad \land \quad (T^n)^m = T^{nm}
\end{align*}
com $m, n \in \mathbb{Z}$ se $T$ é invertível e $m, n \in \mathbb{N}$ caso contrário. Se $T \in \mathcal{L}(V)$ e $p \in \mathcal{P}(\K)$ é um polinômio dado por
\begin{align*}
    p(z) = \sum_{j=1}^n a_jz^j,\: z \in \K
\end{align*}
então $p(T)$ é o operador definido por
\begin{align*}
    p(T) = \sum_{j=1}^n a_jT^j
\end{align*}
Fixado um $T \in \mathcal{L}(V)$, a função
\begin{align*}
    \textbf{T}: \mathcal{P}(\K) &\to \mathcal{L}(V) \\
    p &\mapsto p(T)
\end{align*}
é linear. Se $p, q \in \mathcal{P}(\K)$, então $pq$ é o polinômio definido por
\begin{align*}
    (pq)(z) = p(z)q(z)
\end{align*}
É fácil verificar que
\begin{align*}
    (pq)(T) = p(T)q(T),\: T \in \mathcal{L}(V)
\end{align*}



\newpage

\section{Espaço de Produto Interno}

\subsection{Motivação}

A motivação do conceito de produto interno vem da norma. Como a norma não é linear, injetamos linearidade nela com o produto interno. Definimos o produto escalar nos vetores do $\mathbb{R}^n$
\begin{align*}
    x,y \in \mathbb{R}^n \implies x \cdot y = \sum_{i=1}^n x_iy_i
\end{align*}
e podemos definir a norma a partir disso
\begin{align*}
    \norm{x}^2 = x \cdot x
\end{align*}
Note que o produto escalar satisfaz, $\forall x,y \in \mathbb{R}^n$
\begin{itemize}
    \item $x \cdot x \geq 0$ e $x \cdot x = 0 \iff x = 0$
    \item $\cdot: \mathbb{R}^{n} \to \mathbb{R}^n$ é linear, $x \cdot y = x^Ty$
    \item $x \cdot y = y \cdot x$
\end{itemize}
O produto interno é uma generalização do produto escalar. Antes das definições, precisamos analisar o caso complexo. Para os vetores de $\mathbb{C}^n$, temos
\begin{align*}
    z \in \mathbb{C}^n \implies \norm{z}^2 = \sum_{j=1}^n z_j\overline{z_j} 
\end{align*}
Como queremos que $\norm{z}^2 = z \cdot z$, o produto interno de $z, w \in \mathbb{C}^n$ deve ser algo da forma
\begin{align*}
    z_1\overline{w_1} + \cdots + z_n\overline{w_n}
\end{align*}
Note que se queremos o produto interno de $w$ e $z$, a expressão acima é substituída pelo seu conjugado. Com essas motivações, podemos definir produtos internos sobre $\K$-espaços, com $\K = \mathbb{R}$ ou $\K = \mathbb{C}$.

\subsection{Produto Interno}

Seja $V$ um $\K$-espaço. Um produto interno em $V$ é uma função $\inprod{\cdot}{\cdot} : V \to \K$ que leva um par de vetores $(u,v)$ em $V$ em um escalar $\inprod{u}{v}$ do corpo $\K$, satisfazendo as seguintes propriedades
\begin{itemize}
    \item $\inprod{u}{u} \geq 0,\:\forall u \in V \: \land \: \inprod{u}{u} = 0 \iff u = 0$
    \item $\inprod{u+w}{v} = \inprod{u}{v} + \inprod{w}{v},\:\forall u,v,w \in V$
    \item $\inprod{\alpha u}{v} = \alpha\inprod{u}{v},\:\forall \alpha \in \K,\:\forall u,v \in V$
    \item $\inprod{u}{v} = \overline{\inprod{v}{u}},\:\forall u,v \in V$
\end{itemize}
Note que a condição de simetria conjugada no caso real torna-se apenas simetria. O conjunto $(V, \inprod{\cdot}{\cdot})$ é chamado espaço vetorial de produto interno (\textit{inner-product space}).

\end{document}

